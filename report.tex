\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage {mathtools, graphicx, amsfonts, amssymb, comment}
\usepackage{enumerate}
\usepackage[backend=bibtex,sorting=none]{biblatex}
\usepackage{listings}
\usepackage{hyperref}

\addbibresource{sources.bib}

\title{Finding Roots of Polynomials}
\author{Matt Gaikema}
\date{\today}

\begin{document}

\maketitle

%%%%%%%%%%
% INTRODUCTION %
%%%%%%%%%%

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%
% GENERAL FORMULAS %
%%%%%%%%%%%%%

\section{General Formulas}

If a polynomial is of degree 4 or less, the roots can easily (relatively) be found with explicit formulas.
Galois showed us that there is not an explicit formula for 5th degree polynomials and above.

\subsection{Quadratic Polynomials}

For polynomials of the form $ax^2+bx+c$, the roots are of the form
\begin{equation}
	x = \frac{-b\pm\sqrt{b^2-4ac}}{2a},
\end{equation}
as many people know from seventh or eighth grade. 

\subsection{Cubic Polynomials}

For a polynomial of the form $ax^3+bx^2+cx+d$, the formula for the roots is slightly more complex than that of quadratic polynomials.\cite{wiki:cubic}
\begin{equation}
	x_k=-\frac{1}{3a}\bigg(b+u_kC+\frac{\Delta_0}{u_kC}\bigg),\,k\in{1,2,3},
\end{equation}
where 
\[u_1=1,\,u_2=\frac{-1+i\sqrt{3}}{2},\,u_3=\frac{-1-i\sqrt{3}}{2}\]
are the cubic roots of unity and
\[C=\sqrt[3]{\frac{\Delta_1+\sqrt{\Delta_1^2-4\Delta_0^3}}{2}},\]
with 
\begin{align*}
	\Delta_0 =& b^2-3ac \\
	\Delta_1 =& 2b^3-9abc+27a^2d.
\end{align*}
Also,
\[\Delta_1^2-4\Delta_0^3=27a^2\Delta,\]
where $\Delta$ is the discriminant, $18abcd-4b^3d+b^2c^2-4ac^3-27a^2d^2$.

This formula was probably not taught to most middle schoolers.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%
% NEWTON'S METHOD %
%%%%%%%%%%%%%

\section{Newton's Method}

\begin{equation}
	x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}
\end{equation}

\section{Graeffe's Method}

Graeffe's Method is also known as Graeffe's Root Squaring Method.
For a proof on why Graeffe's Method works, see \cite{graeffe:proof}.

Let $p$ be a polynomial of degree $n$, where 
\[p(x)=(x-\zeta_1)(x-\zeta_2)\dots(x-\zeta_n).\]
Then,
\[p(-x)=(-1)^n(x+\zeta_1)(x+\zeta_2)\dots(x+\zeta_n).\]

Let $q(x)$ be the polynomial with roots $\zeta_1^2,\zeta_2^2,\dots,\zeta_n^2$,
\[q(x)=(x-\zeta_1^2)(x-\zeta_2^2)\dots(x-\zeta_n^2).\]
As shown in \cite{wiki:graeffe}, 
\[q(x^2)=(-1)^np(x)p(-x).\]
$q(x)$ can be computed with operations on the coefficients of $p(x)$.
Let
\begin{align*}
	p(x) =& x^n+a_1x^{n-1}+\dots+a_{n-1}x+a_n \\
	q(x) =& x^n+b_1x^{n-1}+\dots+b_{n-1}x+b_n. 
\end{align*}
Then,
\[b_k=(-1)^ka_k^2+2\sum_{j=0}^{k-1}(-1)^ja_ja_{2k-j},\,a_0=b_0=1.\]

Repeating $k$ times gives a polynomial
\[q^k(y)=y^n+a_1^ky^{n-1}+\dots+a_{n-1}^ky+a_n^k\]
with roots
\[\gamma_1=\zeta_1^{2^k},\gamma_2=\zeta_2^{2^k},\dots,\gamma_n=\zeta_n^{2^k}.\]

Then, Vieta's relations\cite{wiki:vieta} are used: 
\begin{align*}
	a_1^k =& -(\gamma_1+\gamma_2+\dots+\gamma_n) \\
	a_2^k =& \gamma_1\gamma_2+\gamma_1\gamma_3+\dots+\gamma_{n-1}\gamma_n \\
	\vdots & \\
	a_n^k =& (-1)^n(\gamma_1\gamma_2\dots\gamma_n).
\end{align*}

Since the roots are seperated, the first term is larger than the rest.\cite{mathworld:graeffe}
Thus we have
\begin{align*}
	a_1^k \approx& -\gamma_1 \\
	a_2^k \approx& \gamma_1\gamma_2 \\
	\vdots & \\
	a_n^k \approx& (-1)^n(\gamma_1\gamma_2\dots\gamma_n).
\end{align*}
% Finish this

\subsection{An alternate approach}

In his graduate thesis\cite{iterative}, Wankere Mekwi details a similar variant of Graeffe's Method.
\begin{equation}\label{graeffeBase}
	p_0(z)=a_nz^n+a_{n-1}z^{n-1}+\dots+a_0,
\end{equation}
where $a_n=1$.
Equation \ref{graeffeBase} is replaced with a polynomial still of degree $n$ whose roots are the squares of the roots of Equation \ref{graeffeBase}.
\begin{equation}\label{graeffe:p1}
	p_1(z^2)=(-1)^np_0(z)p_0(-z),
\end{equation}
where $p_1$ is the polynomial whose zeros are the squares of the zeros of $p_0$.
This can be repeated such that
\begin{equation}
	p_{r+1}(z^2)=(-1)^np_r(z)p_r(-z)
\end{equation}
is the polynomial has roots which are the squares of the roots of $p_r$.
Each iteration $r$ is referred to as a Graeffe Iteration.\cite{graeffe-iteration}
If the coefficients of $p_r(z)$ are $a_j^{(r)},j=0,1,\dots,n$, then
\begin{equation}
	a_j^{(r+1)}=(-1)^{n-j}\bigg[(a_j^{(r)})^2+2\sum^{\min(n-j,n)}_{k=1}(-1)^ka_{j-k}^{(r)}a_{j+k}^{(r)}\bigg].
\end{equation}
The coeffecients of $p_r$ satisfy
\begin{equation}
	\begin{aligned}
		a_0^{(r)} =& \sigma_0 = 1 \\
		&\vdots \\
		a_j^{(r)} =& (-1)^{n-j}\sigma_{n-j}\big(\zeta_1^{2^r},\zeta_2^{2^r},\dots,\zeta_n^{2^r}\big) \\
		&\vdots \\
		a_{n-1}^{(r)} =& (-1)^{n-1}\sigma_{n-1}\big(\zeta_1^{2^r},\zeta_2^{2^r},\dots,\zeta_n^{2^r}\big) \\
		a_n^{(r)} =& (-1)^n\sigma_n\big(\zeta_1^{2^r},\zeta_2^{2^r},\dots,\zeta_n^{2^r}\big),
	\end{aligned}
\end{equation}
where $\sigma_j$ is the $j$-th Elementary Symetric Polynomial\cite{wiki:symetric},
and the $\zeta_j$ are the roots of $p_0$.
If $|\zeta_1|<|\zeta_2|<\dots<|\zeta_n|$, then we can make an approximation:
\begin{equation}
	\begin{aligned}
		a_0^{(r)} =& \sigma_0 = 1 \\
		&\vdots \\
		a_j^{(r)} \approx& (-1)^{n-j}\zeta_{j+1}^{2^r}\dots\zeta_n^{2^r} \\
		\vdots \\
		a_{n-1}^{(r)} \approx& (-1)^{n-1}\zeta_2^{2^r}\dots\zeta_n^{2^r} \\
		a_n^{(r)} =& (-1)^n\zeta_1^{2^r}\zeta_2^{2^r}\dots\zeta_n^{2^r}.
	\end{aligned}
\end{equation}
Thus we have
\begin{equation}
	\zeta_j^{2^r}\approx-\frac{a_j^{(r)}}{a_{j+1}^{(r)}}.
\end{equation}

\subsection{Effectiveness}

The run-time of Graeffe's Method is $\mathcal{O}(n^2)$ for each iteration, where $n$ is the degree of the polynomial.
Thus $r$ Graeffe Repitions is $\mathcal{O}(rn^2)$.

The biggest weakness of Graeffe's Method is that the coefficients of $p_r$ often become too large for most computers, 
overflowing the floating-point system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%
% ANALYTIC FUNCTIONS %
%%%%%%%%%%%%%%

\section{Analytic Functions}

\subsection{Argument Principle}

According to \cite{delves1967numerical},
if $C$ is a closed curve in the complex plane which does not pass through a zero of $f(z)$, and $R$ is in the interior of $C$, 
then
\begin{equation}\label{eqn:arg-princ}
	s_n=\frac{1}{2\pi i}\oint_Cz^N\frac{f'(z)}{f(z)}dz=\sum_{j=1}^vz_j^N,
\end{equation}
where $z_j,j\in\{1,2,\dots,v\}$ are the zeros of $f$ which lie in $R$.
A multiple zero is counted according to its multiplicity.
When $N=0$, equation \ref{eqn:arg-princ} gives the number of zeros in $C$.
This is a result of the argument principle.\cite{wiki:argument-principle}

Unfortunately, calculating $s_n$ using the Residue Theorem usually involves finding the roots anyways.

\subsection{Numerically Finding Roots}

\cite{delves1967numerical} gives a method, which I will refer to as the "Delves-Lyness Method", for numerically locating the zeros of an analytic function (hence the title),
which has four sections:
\begin{enumerate}
	% 1
	\item Evauluate the number of roots, $s_0=v$, in the region.
	If the number is manageble, calculate $s_1,s_2,\dots,s_v$, and carry on to step (3).
	Otherwise, continue.
	% 2
	\item
	Subdivide the region into smaller subregions and repeat step (1).
	% 3
	\item Given a region and $s_1,s_2,\dots,s_v$, construct and solve the equivalent polynomial $p(z)$.
	% 4
	\item Optionally, take the roots of $p(z)$ as approximations to the roots of $f(z)$,
	and refine these using an iterative method on $f(z)$.
\end{enumerate}
The paper considers two shapes for $R$, squares and circles, 
and  gives three methods for determining $s_N$ in the case of circles.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography

\end{document}